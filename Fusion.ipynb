{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckbFcFLisMjG"
   },
   "outputs": [],
   "source": [
    "# Depression Detection Audio-Video Fusion with Gradio GUI (Colab Ready)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import cv2\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from moviepy.editor import VideoFileClip\n",
    "import gradio as gr\n",
    "\n",
    "# Load models\n",
    "audio_model = tf.keras.models.load_model(\"/models/depression_model_finetuned.h5\")\n",
    "video_model = tf.keras.models.load_model(\"/models/densenet201_depression_model.keras\")\n",
    "scaler = joblib.load(\"/models/scaler.pkl\")\n",
    "\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "FRAMES_PER_VIDEO = 20\n",
    "MAX_PAD_LEN = 216\n",
    "VIDEO_INFLUENCE_WEIGHT = 0.4\n",
    "\n",
    "def extract_frames(video_path, num_frames=FRAMES_PER_VIDEO):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    frames = []\n",
    "    for fid in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            frames.append(np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8))\n",
    "            continue\n",
    "        frame = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    frames = np.array(frames).astype('float32') / 255.0\n",
    "    return frames\n",
    "\n",
    "def extract_audio_features(audio_path, max_pad_len=MAX_PAD_LEN):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=26)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    combined = np.concatenate((mfcc, delta, delta2), axis=0)\n",
    "    pad_width = max(0, max_pad_len - combined.shape[1])\n",
    "    combined = np.pad(combined, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    combined = combined.T[:max_pad_len]\n",
    "    return combined\n",
    "\n",
    "def extract_audio_from_video(video_path):\n",
    "    clip = VideoFileClip(video_path)\n",
    "    temp_audio = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n",
    "    clip.audio.write_audiofile(temp_audio.name, logger=None)\n",
    "    return temp_audio.name\n",
    "\n",
    "def predict_fusion_gradio(video_file):\n",
    "    audio_path = extract_audio_from_video(video_file)\n",
    "    audio_features = extract_audio_features(audio_path)\n",
    "    audio_features = audio_features[np.newaxis, ...]\n",
    "    audio_features_scaled = scaler.transform(audio_features.reshape(-1, audio_features.shape[2])).reshape(audio_features.shape)\n",
    "    audio_pred = audio_model.predict(audio_features_scaled)[0][0]\n",
    "    frames = extract_frames(video_file)\n",
    "    video_preds = video_model.predict(frames)\n",
    "    video_emotion_mean = np.mean(video_preds)\n",
    "    video_adjustment = VIDEO_INFLUENCE_WEIGHT * (video_emotion_mean - 0.5)\n",
    "    final_pred = audio_pred + video_adjustment\n",
    "    final_pred = np.clip(final_pred, 0, 1)\n",
    "    label = \"Depressed\" if final_pred >= 0.5 else \"Not Depressed\"\n",
    "    os.unlink(audio_path)\n",
    "    return f\"Audio: {audio_pred:.3f}, Video: {video_emotion_mean:.3f}, Final: {final_pred:.3f}, Label: {label}\"\n",
    "\n",
    "def launch_gradio():\n",
    "    gr.Interface(\n",
    "        fn=predict_fusion_gradio,\n",
    "        inputs=gr.File(file_types=[\".mp4\", \".flv\"], label=\"Upload MP4 or FLV Video\"),\n",
    "        outputs=\"text\",\n",
    "        title=\"Depression Detection\",\n",
    "        description=\"Upload a video (MP4/FLV) containing both audio and video. The model will predict depression status.\"\n",
    "    ).launch(share=True, debug=True)\n",
    "\n",
    "launch_gradio()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
